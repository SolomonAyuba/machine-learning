{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMU+9Y7rI0cZZ4gfiuwb7mu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SolomonAyuba/machine-learning/blob/main/Machine_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preprocessing Techniques\n",
        "\n",
        "1. ### Rescaling Data (Normalisation)\n",
        "\n",
        "  Rescaling is essential when features have different ranges. For instance, if one\n",
        "  variable ranges from 0 to 1000, while another is between 0 and 1, models that\n",
        "  depend on distance calculations, such as k-Nearest Neighbours (KNN) and Support Vector Machines (SVMs), may become biased.\n",
        "  - Min-Max Scaling: Converts features to a scale between 0 and 1.\n",
        "  - Formular: _X' = X - X_min Ã· X_max - X_min_\n",
        "\n",
        "  Example using Scikit-Learn"
      ],
      "metadata": {
        "id": "jm209AMyBlX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([[200], [400], [600], [800], [1000]])\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "print(scaled_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mEg6DvMCY38",
        "outputId": "22d9df90-73aa-4676-d811-18d1e4538479"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.  ]\n",
            " [0.25]\n",
            " [0.5 ]\n",
            " [0.75]\n",
            " [1.  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. ### Standardisation (Z-Score scaling)\n",
        "\n",
        "  Standardisation transforms data so that it has a mean of 0 and a standard\n",
        "  deviation of 1. This technique is crucial for models such as logistic regression and linear regression, which assume normally distributed data.\n",
        "  - Z-Score Formula: _X' = X - Î¼ Ã· Ïƒ_\n",
        "  - where Î¼ is the mean and Ïƒ is the standard deviation.\n",
        "  \n",
        "  Example using Scikit-Learn"
      ],
      "metadata": {
        "id": "ma_Bh2pvFhr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([[50], [100], [150], [200], [250]])\n",
        "scaler = StandardScaler()\n",
        "standardized_data = scaler.fit_transform(data)\n",
        "print(standardized_data)"
      ],
      "metadata": {
        "id": "hHTJjHdmGldt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. ### Binarisation\n",
        "  Binarisation transforms data into binary values, often required for algorithms\n",
        "  like Bernoulli Naive Bayes.\n",
        "\n",
        "  Example using Scikit-Learn"
      ],
      "metadata": {
        "id": "8LhllBHWH0sL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import Binarizer\n",
        "\n",
        "data = np.array([[1.5], [0.3], [2.8], [0.5]])\n",
        "binarizer = Binarizer(threshold=1.0)\n",
        "binary_data = binarizer.fit_transform(data)\n",
        "print(binary_data)"
      ],
      "metadata": {
        "id": "RN_WhxN3HVid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Selection in Machine Learning\n",
        "Feature selection aims to identify the most relevant variables for model training, eliminating redundant or irrelevant ones.\n",
        "\n",
        "## Methods of feature selection\n",
        "**ðŸ“Œ Univariate selection:** Uses statistical tests to evaluate feature\n",
        "importance."
      ],
      "metadata": {
        "id": "UHm-C2EIJG9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklaern.datasets import load_iris\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "best_features = SelectKBest(score_func=f_classif, k=2)\n",
        "X_new = best_featires.fit_transform(X, y)\n",
        "print(X_new.shape)"
      ],
      "metadata": {
        "id": "DjpbXGDCIr4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ðŸ“Œ Principal Component Analysis (PCA):** Reduces dimensionality while\n",
        "preserving variance."
      ],
      "metadata": {
        "id": "ghFolGhPJ-OG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "print(X_pca.shape)"
      ],
      "metadata": {
        "id": "KImKnFvkJ08D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Testing in Machine Learning\n",
        "The goal of a machine learning model is to learn patterns from data and generalise to new, unseen inputs. This requires a structured approach to model evaluation.\n",
        "\n",
        "##Why do we need separate training and testing sets?\n",
        "- **To avoid overfitting:** If a model is tested on the same data it was trained on, it may simply memorise patterns instead of generalising.\n",
        "- **To measure real-world performance:** A model should be evaluated on unseen data to understand how it will behave in practical applications.\n",
        "- **To compare models effectively:** Different models can be assessed fairly by using the same test dataset for evaluation.\n",
        "\n",
        "## Splitting Data for Training and Testing\n",
        "### Holdout method\n",
        "The simplest technique is to split the dataset into two parts:\n",
        "1. Training set: Used to train the model.\n",
        "2. Testing set: Used to evaluate performance.\n",
        "\n",
        "ðŸ”¹ Common split ratios:\n",
        "- 80% training / 20% testing\n",
        "- 70% training / 30% testing\n",
        "- 67% training / 33% testing (used in small datasets)\n",
        "\n",
        "**Advantage:** Simple and fast.\n",
        "**Disadvantage:** Results can vary significantly depending on the split.\n",
        "\n",
        "Example using Scikit-learn"
      ],
      "metadata": {
        "id": "DmPGD6TvMpK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linera_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split Dataset (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_triain)\n",
        "\n",
        "# EValuate model\n",
        "accuracy = model.score(X_test, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "eeN3MEF9KO_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Splitting Data for Training and Testing\n",
        "###k-Fold Cross-Validation\n",
        "Instead of using a single train-test split, k-fold cross-validation divides the\n",
        "dataset into k equal parts (folds). The model is trained on k-1 folds and tested\n",
        "on the remaining fold. This process is repeated k times, with each fold used as\n",
        "a test set once.\n",
        "\n",
        "####Common k values:\n",
        "- k = 5 (Standard for most datasets)\n",
        "- k = 10 (Used when more training data is required)\n",
        "- Leave-One-Out Cross-Validation (LOOCV) (Extreme case where k =\n",
        "number of instances)\n",
        "\n",
        "**Advantage:** More reliable than a single train-test split.\n",
        "**Disadvantage:** It is computationally expensive.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "jAzTgIIlPVX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Perform 5-fold cross-validation\n",
        "scores= cross_val_score(model, X, y, cv=5)\n",
        "print(f\"Cross-validation Accuracy: {scores.mean():.2f}\")\n"
      ],
      "metadata": {
        "id": "7kBataScOdxv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}